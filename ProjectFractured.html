<!DOCTYPE html>
<html data-wf-page="61c5e06d4c32b4d376aaf502" data-wf-site="603fe89ec4bbcece7ee6a53a" lang="en">

<head>
  <meta charset="utf-8" />
  <title>Negotiated Intelligence | The Wrong Biennale 7th Edition</title>
  <meta content="Negotiated Intelligence" name="description" />
  <meta content="Negotiated Intelligence" property="og:title" />
  <meta content="Negotiated Intelligence" property="og:description" />
  <meta property="og:type" content="website" />
  <meta content="width=device-width, initial-scale=1" name="viewport" />
  <link href="base.css" rel="stylesheet" type="text/css" />
  <link href="assets/img/favicon.png" rel="shortcut icon" type="image/x-icon" />
  <link href="assets/img/favicon.png" rel="apple-touch-icon" />
  <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=603fe89ec4bbcece7ee6a53a" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.6.1/gsap.min.js"></script>

  <script type="text/javascript">
    ! function(o, c) {
      var n = c.documentElement,
        t = " w-mod-";
      n.className += t + "js", ("ontouchstart" in o || o.DocumentTouch && c instanceof DocumentTouch) && (n.className += t + "touch")
    }(window, document);
  </script>

  <style>
    .collection {
      opacity: 0;
    }

    .info-img-gradient.bottom {
      transform: scaleY(-1);
    }
  </style>

  <style>
    *,
    *::after,
    *::before {
      box-sizing: border-box;
    }

    html {
      background: #030303;
    }

    body {
      -webkit-tap-highlight-color: transparent;
      -webkit-font-smoothing: subpixel-antialiased;
      -webkit-font-smoothing: antialiased;
      -moz-osx-font-smoothing: grayscale;
      -webkit-overflow-scrolling: touch;
      -webkit-overflow-scrolling: touch;
      scrollbar-width: none;
      overscroll-behavior: none;
    }

    .content {
      position: relative;
      z-index: 2;
    }


    /*Disable Text Selection */
    @media (min-width: 991px) {
      * {
        -webkit-touch-callout: none;
        -webkit-user-select: none;
        -moz-user-select: none;
        -ms-user-select: none;
        user-select: none;
      }
    }

    input,
    textarea {
      -webkit-user-select: text;
      -khtml-user-select: text;
      -moz-user-select: text;
      -ms-user-select: text;
      user-select: text;
    }

    /*Remove Scrollbar */
    ::-webkit-scrollbar {
      -ms-overflow-style: none;
      display: none;
      overflow: -moz-scrollbars-none;
      scrollbar-width: none;
      -ms-overflow-style: none;
      -ms-overflow-style: -ms-autohiding-scrollbar;
    }

    .grid-w {
      display: none;
    }

    .underline-style {
      text-underline-offset: 0.3rem;
      text-decoration-thickness: 1px;
    }

    .blend_back {
      background-blend-mode: luminosity;
      background-color: #030303;
      opacity: 0.1;
      display: block;
    }

    canvas {
      left: 0;
      position: fixed;
      top: 0;
      z-index: 1;
    }

    [data-animation="slider"],
    [data-animation="slider"] .collection-list {
      display: inline-block;
      width: auto !important;
      white-space: nowrap;
    }

    [data-animation="slider-item"] {
      display: inline-block !important;
      height: 38vw;
      position: relative;
      width: 55vw;
    }

    [data-animation="slider-item"] img {
      border: 2px solid #bbb9b94a;
      border-radius: 18px;
      position: absolute;
      object-fit: cover;
      left: 0;
      top: 0;
      height: 100%;
      width: calc(100% - 2vw);
    }

    .menu {
      height: 100vh;
      height: calc(var(--vh, 1vh) * 100);
    }

    .loader {
      display: block;
    }

    .hero.home,
    .wait {
      height: 100vh;
      /* height: calc(var(--vh, 1vh) * 100); */
    }

    audio {
      opacity: 0.8;
    }

    audio::-webkit-media-controls-panel {
      background-color: #535353;
    }

    audio::-webkit-media-controls-current-time-display {
      text-shadow: none;
      color: white;
    }

    audio::-webkit-media-controls-time-remaining-display {
      text-shadow: none;
      color: white;
    }

    .dataset .h-h2.desc {
      font-size: 1.5rem;
      line-height: 1.3;
      font-weight: 300;
      border-bottom: 1px solid #ddd;
      padding: 8px;
    }

    thead {
      background-color: #585858;
      color: white;
      font-weight: 300;
    }

    th,
    td {
      border-bottom: 1px solid #ddd;
      padding: 8px;
    }

    table {
      border-collapse: collapse;
      border-top: 2px solid black;
      border-bottom: 2px solid black;
    }
  </style>

</head>

<body class="body" data-ix="pre-loader">
  <div class="gradient-embed w-embed">
    <style>
      .gradient {
        -webkit-text-fill-color: transparent;
        -webkit-background-clip: text;
      }

      .blend {
        mix-blend-mode: difference;
      }
    </style>
  </div>

  <div class="navbar navbar__shop" style="display:none; padding:1rem 0.5vw 0 2.5vw ">
    <div class="n-block left">
      <div data-w-id="2264dfe2-412c-a750-b778-fffb7eb6b996" class="n-wrap">
        <div data-w-id="2264dfe2-412c-a750-b778-fffb7eb6b997" class="n-trigger">
          <div data-w-id="2264dfe2-412c-a750-b778-fffb7eb6b998" class="n-line"></div>
          <div data-w-id="2264dfe2-412c-a750-b778-fffb7eb6b999" class="n-line bottom"></div>
        </div>
      </div>
    </div>

    <div class="n-block center"><a rel="noopener" draggable="false" href="/" aria-current="page" class="n-link w-inline-block w--current"><img style="opacity: 0.9; width: 200px;"
          src="https://freight.cargo.site/w/217/q/94/i/fb2b46cd634f8600d388c1fddfc23c48e2c1e3593988f576bbec4237426c8fcb/CAMLab.png" loading="lazy" class="n-brand" /></a>
    </div>
    <div class="n-block right"></div>
  </div>

  <main data-template="home" class="content demo">

    <div id="dataset" class="collection" style="margin-top: 0vw">

      <div class="campaign">
        <div style="text-align: center; " class="lookbook__head__title">
          <h2 style=" line-height: 1.1; padding-top: 2rem;" class="h-h2 gradient">
            Fractured Star Projections
          </h2>
          <div class="hero-text">Aaron Oldenburg</div>
        </div>
        <div class="intro" id="intro" style="padding: 5% 15% 0;">
          <div class="intro-inner">
            <div class="intro-desc">
              <h2 class="h-h2 desc" style="line-height: 1.5;text-align: left;">
              </h2>
            </div>
          </div>
        </div>

        <div class="intro" style="padding: 5% 15% 0">
          <div class="intro-inner">
            <div class="intro-desc">
              <h2 class="h-h2 desc" style="line-height: 1.5; margin-bottom: 1rem; text-align: left;">
                Human Creator Ledger
              </h2>
              <hr>
              <div style="line-height: 1.3; font-size: 1.2rem;  text-align: left;">
                <br>
                <b>Pre-AI concept</b>
                <br><br>
                <p>

                  <!-- PDF Embed -->
                  <!-- <embed style="width:100%; height:800px;" class="pdf-embed" src="assets/pdf/Aaron_initial_writings_annotated.pdf" type="application/pdf"> -->

                  <object style="width:100%; height:800px;" class="pdf-embed" data="assets/pdf/Aaron_initial_writings_annotated.pdf#toolbar=0&navpanes=0&scrollbar=0" type="application/pdf">
                    <p>Your browser does not support PDFs. Please update your browser.</p>
                  </object>
                <p><i> Annotated documentation of pre-AI concept. Unrelated notes are blacked-out. Notes that are
                    related but potentially embarrassing, as they were written down without thought of
                    publication, were left in.
                  </i></p>
                <br><br>
                <p>
                  This project arose out of an interest in playing with what other people have asked AI
                  to do. On Midjourney's Discord channel, there is an archive of everything publicly
                  generated by users, alongside the prompts that they wrote and used. I had a
                  sociological/cultural interest in this, but I was also curious to try using the creative
                  constraints of not asking generative AI to make things to my specifications, but
                  relying on what other people asked of it. Having played with AI generation, myself, I
                  know that prompts vary from the purposeful and utilitarian to the spontaneous,
                  personal and revealing. In this way, it is kind of like looking at someone's web
                  searches, but with different imaginative boundaries.

                  <br><br>

                  There are other reasons why using the results of other people's generative AI
                  prompts was appealing to me. One is that it has almost no additional environmental
                  impact compared to using my own new prompts to generate images. At this time, the pursuit of "found" AI has not created incentives for the creation of more AI-generated
                  images.
                  <br><br>
                  There is still the issue of AI plagiarizing others' work, which this process, on its own,
                  does not solve. However, in this project, I have chosen not to show the generated
                  works, themselves, but to use those images as writing, drawing, and audio prompts
                  for an interactive narrative. Although players will have access to an archive of these
                  images for reference, they, themselves, are not part of the final work.
                  <br><br>
                  In a nod to the plagiaristic nature of AI, I have included some stolen text of my own,
                  though: that of the prompts, which I occasionally weave into my narrative (the title,
                  itself, was taken from a prompt). I don't think I benefit unduly from this theft, as the
                  writing in prompts is not great. I considered visually highlighting the borrowed text,
                  but felt it distracting. I do make all of the prompts available in a text file separate from
                  the work.
                  <br><br>
                  I chose images based on intuitive pull. Most were dumb. Many were obsessive, with
                  the user trying double-digit attempts to get the perfect image with variations on a
                  prompt. With those, I often downloaded the entire series of images. I was not looking
                  for exciting or polished. Sometimes the pull for me was wondering what drove the
                  person toward that particular prompt. Sometimes it was the absurd amount of
                  specificity. Often, the work generated was not aesthetically interesting but the
                  prompt, itself, and the obsessive iterations, were.
                  <br><br>
                  I began world-building off of the descriptions of the work. I enjoyed playing with the
                  image variation and incorporating that slipperiness into my narrative. The narrative is
                  absurd and free-associative.
                  <br><br>
                  This was my first Twine project. Twine is a software that allows one to quickly build
                  interactive narratives and publish them as HTML. It gives one a nice bird's eye view
                  of what quickly becomes a sprawling flowchart. This stretched my creative writing
                  process.
                  <br><br>
                  I originally began with the intention of pairing my text with the images. The goal was
                  to undermine and reinterpret the images. However, Twine is not an ideal engine for
                  image representation, and I found that I preferred reading the text without the image.
                  With the image-less text removing us from what the prompt generated, we are now

                  required to create our own images in our heads (a process that should, of course, be
                  familiar to readers).
                  <br><br>
                  <b>
                    I feel like I am collaborating with the reader to organically recreate these
                    images that have been blandly and procedurally created via AI. I also like how
                    this process draws attention to how the human brain creates images, and what
                    similarities, if any, exist between our image creation and that of a digital neural
                    network.
                  </b> <br><br>
                  To keep the focus on this process, I did include some images in the final product. I
                  made some of my own drawings of the AI-generated images. To do this, I used the
                  old drawing class technique where one looks only at the object being drawn and
                  never at the paper on which one is drawing. What one ends up with are loose
                  scribbles but also a sense of really seeing the contours of what one is looking at. The
                  process of creating drawings this way put my attention on how my brain takes in the
                  information from the image and translates that into arm movement. The vagueness
                  of the images also complements the narrative without hijacking it in the same way
                  inclusion of the original generated images would have.<br><br>
                  I also responded to some images with sound effects, music, and narration that I
                  made. These appear on a handful of nodes.<br><br>
                  Some of the narrative themes also touch on how our brains generate reality. The
                  story does not really end. Most of the threads stop with a choice to jump to an earlier
                  part of the narrative and explore different paths.<br><br>



              </div>
            </div>
          </div>
        </div>

        <div data-animation="blendmode" class="coll-wrap" style="margin: 2rem;">
          <img src="assets/img/fractured/example_05.png" loading="lazy" sizes="(max-width: 2219px) 100vw, 2219px" srcset="assets/img/fractured/example_05.png 500w,
                                                      assets/img/fractured/example_05.png 800w,
                                                                assets/img/fractured/example_05.png 1080w,
                                                                assets/img/fractured/example_05.png 1600w,
                                                                assets/img/fractured/example_05.png 2000w,
                                                            assets/img/fractured/example_05.png 2219w" alt="" class="tab__image__media seamless">
          <!-- <div class="coll-gradient"></div> -->
          <div class="info-img-gradient up"></div>
          <div class="info-img-gradient bottom"></div>
          <div class="invert w-embed"></div>
        </div>

        <div data-animation="blendmode" class="coll-wrap" style="margin: 2rem;">
          <img src="assets/img/fractured/example_08.png" loading="lazy" sizes="(max-width: 2219px) 100vw, 2219px" srcset="assets/img/fractured/example_08.png 500w,
                                                      assets/img/fractured/example_08.png 800w,
                                                                assets/img/fractured/example_08.png 1080w,
                                                                assets/img/fractured/example_08.png 1600w,
                                                                assets/img/fractured/example_08.png 2000w,
                                                            assets/img/fractured/example_08.png 2219w" alt="" class="tab__image__media seamless">
          <!-- <div class="coll-gradient"></div> -->
          <div class="info-img-gradient up"></div>
          <div class="info-img-gradient bottom"></div>
          <div class="invert w-embed"></div>
        </div>
        <!-- <div>Intermediate video frame 2</div> -->
        <div>Screenshot from Twine for building <i>Fractured Star Projections</i></div>




        <div class="intro" style="padding: 5% 15% 0">
          <div class="intro-inner">
            <div class="intro-desc">
              <h2 class="h-h2 desc" style="line-height: 1.5; margin-bottom: 1rem; text-align: left;">
                AI Co-Creator Profile
              </h2>
              <hr>
              <div style="line-height: 1.3; font-size: 1.2rem;  text-align: left;">
                <br>
                <p>
                  I used Midjourney. It was a software that I had experience using in the past. The
                  most important aspect of the software for this project was the public archive of past
                  prompts and outputs. <br><br>This project is different from most AI collaborations, as usually the artist is also the
                  one who writes the prompts. There is an enjoyment inherent in that space between
                  what one writes and what is output from the system. In this project, I see the end
                  results of that. Often, the reverse is surprising: I see the image initially, and
                  subsequently get a glimpse of the human behind the image by reading the prompt.
                  What they wanted versus what they received is often made painfully clear through
                  the number of iterations they made on the prompts.
                  <br><br>
                  I wove these iterations into the narrative. Often the character will have unexplained
                  changes. Some of these are mundane, such as the color and style of the character's
                  sweater changing in each description. Others play with the fact that AI doesn't
                  understand the physical reality of the scenes being depicted, and so the world I
                  create often abides by the rules of the AI generated images rather than those of
                  physical reality: disappearing hands, a floor that drips onto you after a bath.
                  <br><br>
                  There is a danger here of falling into cliche surrealism. But I think the process gets at
                  something surrealists were engaged in more deeply: attempting to find and inhabit
                  an underlying reality. This reality is metaphorically expressed through the opacity of
                  generative AI's code and training data.<br><br>
                  Although the images, themselves, are not in the final piece, the process that
                  generates them inhabits my description and narrative. Sometimes I describe what I
                  see in the images fairly faithfully, and other times I use part of the image simply as a
                  launching point in an original narrative vignette.<br><br>
                  All of this is in service to the old collaborative process between author and reader,
                  where we join together to generate ephemeral images in our biological neural
                  networks. Using AI helped me shine a light on this process.
                  <br><br>

                </p>
              </div>
            </div>
          </div>
        </div>



        <div data-animation="blendmode" class="coll-wrap" style="margin: 2rem;">
          <img src="assets/img/fractured/Twine_01.png" loading="lazy" sizes="(max-width: 2219px) 100vw, 2219px" srcset="assets/img/fractured/Twine_01.png 500w,
                                                                        assets/img/fractured/Twine_01.png 800w,
                                                                                  assets/img/fractured/Twine_01.png 1080w,
                                                                                  assets/img/fractured/Twine_01.png 1600w,
                                                                                  assets/img/fractured/Twine_01.png 2000w,
                                                                              assets/img/fractured/Twine_01.png 2219w" alt="" class="tab__image__media seamless">
          <!-- <div class="coll-gradient"></div> -->
          <div class="info-img-gradient up"></div>
          <div class="info-img-gradient bottom"></div>
          <div class="invert w-embed"></div>
        </div>
        <!-- <div>Intermediate video frame 2</div> -->



        <div data-animation="blendmode" class="coll-wrap" style="margin: 2rem;">
          <img src="assets/img/fractured/Twine_02.png" loading="lazy" sizes="(max-width: 2219px) 100vw, 2219px" srcset="assets/img/fractured/Twine_02.png 500w,
                                assets/img/fractured/Twine_02.png 800w,
                                          assets/img/fractured/Twine_02.png 1080w,
                                          assets/img/fractured/Twine_02.png 1600w,
                                          assets/img/fractured/Twine_02.png 2000w,
                                      assets/img/fractured/Twine_02.png 2219w" alt="" class="tab__image__media seamless">
          <!-- <div class="coll-gradient"></div> -->
          <div class="info-img-gradient up"></div>
          <div class="info-img-gradient bottom"></div>
          <div class="invert w-embed"></div>
        </div>
        <!-- <div>Intermediate video frame 2</div> -->





        <div data-animation="blendmode" class="coll-wrap" style="margin: 2rem;">
          <img src="assets/img/fractured/Twine_03.png" loading="lazy" sizes="(max-width: 2219px) 100vw, 2219px" srcset="assets/img/fractured/Twine_03.png 500w,
                                        assets/img/fractured/Twine_03.png 800w,
                                                  assets/img/fractured/Twine_03.png 1080w,
                                                  assets/img/fractured/Twine_03.png 1600w,
                                                  assets/img/fractured/Twine_03.png 2000w,
                                              assets/img/fractured/Twine_03.png 2219w" alt="" class="tab__image__media seamless">
          <!-- <div class="coll-gradient"></div> -->
          <div class="info-img-gradient up"></div>
          <div class="info-img-gradient bottom"></div>
          <div class="invert w-embed"></div>
        </div>
        <!-- <div>Intermediate video frame 2</div> -->





        <div data-animation="blendmode" class="coll-wrap" style="margin: 2rem;">
          <img src="assets/img/fractured/Twine_04.png" loading="lazy" sizes="(max-width: 2219px) 100vw, 2219px" srcset="assets/img/fractured/Twine_04.png 500w,
                                                                assets/img/fractured/Twine_04.png 800w,
                                                                          assets/img/fractured/Twine_04.png 1080w,
                                                                          assets/img/fractured/Twine_04.png 1600w,
                                                                          assets/img/fractured/Twine_04.png 2000w,
                                                                      assets/img/fractured/Twine_04.png 2219w" alt="" class="tab__image__media seamless">
          <!-- <div class="coll-gradient"></div> -->
          <div class="info-img-gradient up"></div>
          <div class="info-img-gradient bottom"></div>
          <div class="invert w-embed"></div>
        </div>
        <!-- <div>Intermediate video frame 2</div> -->
        <div>Screenshot from Twine for building <i>Fractured Star Projections</i></div>




        <div class="intro" style="padding: 5% 15% 0">
          <div class="intro-inner">
            <div class="intro-desc">
              <h2 class="h-h2 desc" style="line-height: 1.5; margin-bottom: 1rem; text-align: left;">
                Synthesis Process
              </h2>
              <div style="line-height: 1.3; font-size: 1.2rem;  text-align: left;">
                <p>
                  I enjoyed the flow of going in and out of descriptive writing based on the picture I was
                  viewing, as loosely or as closely as I felt.
                  <br><br>
                  The process illuminated the contrast between the AI's limitations, seen in the back-
                  and-forth between prompter and generator, and what felt like my own lack lack of any

                  imaginative limitations. It was easy to write narrative that was constrained by the
                  images I happened to find. This did make it hard to tidily wrap up the eventually
                  sprawling narrative, let alone guide it toward any conventional plot.

                  I found this to be an inspiring creative exercise and have had thoughts of creating
                  additional works using found AI, or creating a game jam with this theme.

                </p>
              </div>
            </div>
          </div>
        </div>


        <div data-animation="blendmode" class="coll-wrap" style="margin: 2rem;">
          <img src="assets/img/fractured/screenshot_01.png" loading="lazy" sizes="(max-width: 2219px) 100vw, 2219px" srcset="assets/img/fractured/screenshot_01.png 500w,
                                                                assets/img/fractured/screenshot_01.png 800w,
                                                                          assets/img/fractured/screenshot_01.png 1080w,
                                                                          assets/img/fractured/screenshot_01.png 1600w,
                                                                          assets/img/fractured/screenshot_01.png 2000w,
                                                                      assets/img/fractured/screenshot_01.png 2219w" alt="" class="tab__image__media seamless">
          <!-- <div class="coll-gradient"></div> -->
          <div class="info-img-gradient up"></div>
          <div class="info-img-gradient bottom"></div>
          <div class="invert w-embed"></div>
        </div>
        <div>Screenshot from <i>Fractured Star Projections</i></div>






      </div>
    </div>


  </main>

  </main>
  <div class="grid-event w-embed">
    <style>
      .grid-w {
        pointer-events: none;
      }
    </style>
  </div>
  <script src="./composition_master.js" type="text/javascript"></script>

  <script src="assets/js/player.js"></script>

  <script>
    //100vh Fix on Mobile
    let vh = window.innerHeight * 0.01;
    let size = 0;
    document.documentElement.style.setProperty('--vh', `${vh}px`);
    window.addEventListener('resize', () => {
      if (window.innerWidth === size) return;
      size = window.innerWidth;
      let vh = window.innerHeight * 0.01;
      document.documentElement.style.setProperty('--vh', `${vh}px`);
    });

    TweenMax.to(".collection", 10, {
      opacity: 1
    });
  </script>

</body>


</html>
